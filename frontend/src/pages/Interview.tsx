import { useState, useRef, useEffect } from "react";
import { useNavigate } from "react-router-dom";
import { Button } from "@/components/ui/button";
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { Progress } from "@/components/ui/progress";
import { Badge } from "@/components/ui/badge";
import { Textarea } from "@/components/ui/textarea"; // Textarea ì¶”ê°€
import {
  Mic, MicOff, Video, VideoOff, Play, Pause,
  SkipForward, Settings, HelpCircle, Clock
} from "lucide-react";
import { useToast } from "@/hooks/use-toast";
import { useAuth } from "@/contexts/AuthContext"; // âœ… ìœ ì € ì •ë³´ ê°€ì ¸ì˜¤ê¸°
import axios from "axios";
import io, { Socket } from "socket.io-client";

const BASE_URL = import.meta.env.VITE_API_BASE_URL;

const Interview = () => {
  const navigate = useNavigate();
  const { toast } = useToast();
  const videoRef = useRef<HTMLVideoElement>(null);
  const { user } = useAuth(); // âœ… ë¡œê·¸ì¸ëœ ìœ ì € ì •ë³´

  const [currentQuestion, setCurrentQuestion] = useState(0);
  const [isRecording, setIsRecording] = useState(false);
  const [isVideoOn, setIsVideoOn] = useState(true);
  const [isMicOn, setIsMicOn] = useState(true);
  const [timeRemaining, setTimeRemaining] = useState(180);
  const [transcription, setTranscription] = useState("");
  const [isInterviewStarted, setIsInterviewStarted] = useState(false);
  const [questions, setQuestions] = useState<string[]>([]);
  const [isLoading, setIsLoading] = useState(true);
  const [sentimentScore, setSentimentScore] = useState(100);
  const [interviewId, setInterviewId] = useState<number | null>(null);
  const [isReadyToAnalyze, setIsReadyToAnalyze] = useState(false);

  const wsRef = useRef<WebSocket | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);
  const socketRef = useRef<Socket | null>(null);

  useEffect(() => {
    const fetchQuestions = async () => {
      if (!user) {
        setIsLoading(false);
        setQuestions(["ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ë¡œê·¸ì¸ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."]);
        return;
      }
      try {
        const response = await axios.get(`/api/interview/questions/${user.id}`);
        if (response.data.questions && response.data.questions.length > 0) {
          setQuestions(response.data.questions);
        } else {
          setQuestions([]); // ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •
          toast({
            title: "ìƒì„±ëœ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤",
            description: "ì„¤ì • í˜ì´ì§€ì—ì„œ ë¨¼ì € ì›í•˜ëŠ” ê¸°ì—…ì˜ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
            duration: 5000,
          });
        }
      } catch (error) {
        console.error("ì§ˆë¬¸ ë¡œë”© ì‹¤íŒ¨:", error);
        setQuestions(["ì§ˆë¬¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨ í•´ì£¼ì„¸ìš”."]);
        toast({
          title: "ì§ˆë¬¸ ë¡œë”© ì‹¤íŒ¨",
          description: "ì„œë²„ì—ì„œ ì§ˆë¬¸ì„ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
          variant: "destructive",
        });
      } finally {
        setIsLoading(false);
      }
    };

    fetchQuestions();
  }, [user, toast]);

  useEffect(() => {
    let interval: NodeJS.Timeout;
    if (isRecording && timeRemaining > 0) {
      interval = setInterval(() => {
        setTimeRemaining((prev) => prev - 1);
      }, 1000);
    } else if (timeRemaining === 0) {
      handleNextQuestion();
    }
    return () => clearInterval(interval);
  }, [isRecording, timeRemaining]);

  useEffect(() => {
    const setupCamera = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
        if (videoRef.current) videoRef.current.srcObject = stream;
      } catch (error) {
        console.error("Camera setup failed:", error);
        toast({
          title: "ì¹´ë©”ë¼ ì ‘ê·¼ ì‹¤íŒ¨",
          description: "ì¹´ë©”ë¼ì™€ ë§ˆì´í¬ ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
          variant: "destructive"
        });
      }
    };
    if (isVideoOn) setupCamera();
  }, [isVideoOn, toast]);

  // Sentiment analysis frame capture logic
  useEffect(() => {
    if (!isReadyToAnalyze || !isRecording) return;

    const canvas = document.createElement('canvas');
    const context = canvas.getContext('2d');

    const analysisInterval = setInterval(async () => {
      if (!isRecording || !videoRef.current || !context) {
        return;
      }

      canvas.width = videoRef.current.videoWidth;
      canvas.height = videoRef.current.videoHeight;
      context.drawImage(videoRef.current, 0, 0, canvas.width, canvas.height);
      const image = canvas.toDataURL('image/jpeg');

      try {
        if (user && interviewId !== null) {
          await axios.post(`/api/interview/analyze-frame`, { 
            image,
            interviewId,
            questionNumber: currentQuestion + 1,
            userId: user.id
          });
        }
      } catch (error) {
        console.error("Frame analysis failed:", error);
      }
    }, 1000); // 1ì´ˆ(1000ms)ë§ˆë‹¤ ë¶„ì„ ì‹¤í–‰

    return () => {
      clearInterval(analysisInterval);
    };
  }, [isRecording, isReadyToAnalyze, user, interviewId, currentQuestion]);

  if (isLoading) {
    return (
      <div className="flex items-center justify-center h-screen">
        <div className="animate-spin rounded-full h-16 w-16 border-t-2 border-b-2 border-slate-900"></div>
        <p className="ml-4 text-lg">ì§ˆë¬¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...</p>
      </div>
    );
  }

  const startSpeechRecognition = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaStreamRef.current = stream;

      const audioContext = new AudioContext();
      await audioContext.audioWorklet.addModule("/audio-processor.js");

      const source = audioContext.createMediaStreamSource(stream);
      const workletNode = new AudioWorkletNode(audioContext, "audio-processor");

      wsRef.current = new WebSocket("ws://localhost:8765");

      wsRef.current.onopen = () => {
        console.log("ğŸ™ï¸ STT WebSocket ì—°ê²°ë¨");

        workletNode.port.onmessage = (event) => {
          const float32Data = new Float32Array(event.data);
          if (wsRef.current?.readyState === WebSocket.OPEN) {
            wsRef.current.send(float32Data.buffer);
          }
        };

        source.connect(workletNode).connect(audioContext.destination);
      };

      wsRef.current.onmessage = (event) => {
        const text = event.data;
        if (text) {
          setTranscription((prev) => prev + " " + text);
        }
      };

      wsRef.current.onerror = (err) => {
        console.error("STT WebSocket ì—ëŸ¬:", err);
      };

      wsRef.current.onclose = () => {
        console.log("ğŸ”Œ STT ì—°ê²° ì¢…ë£Œ");
      };

    } catch (err) {
      console.error("ğŸ¤ ìŒì„± ì¸ì‹ ì‹¤íŒ¨:", err);
      toast({
        title: "ìŒì„± ì¸ì‹ ì‹¤íŒ¨",
        description: "ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.",
        variant: "destructive"
      });
    }
  };

  const stopSpeechRecognition = () => {
    // STT ë¹„í™œì„±í™”ë¡œ ë‚´ìš© ì£¼ì„ ì²˜ë¦¬
    // processorRef.current?.disconnect();
    // mediaStreamRef.current?.getTracks().forEach((track) => track.stop());
    // wsRef.current?.close();
    // processorRef.current = null;
    // mediaStreamRef.current = null;
    // wsRef.current = null;
  };

  const startInterview = async () => {
    if (!user) return;

    try {
      const response = await axios.post(`/api/interview/start`, { user_id: user.id });
      if (response.data && response.data.interview_id) {
        setInterviewId(response.data.interview_id);
      }
      
      setIsInterviewStarted(true);
      setIsRecording(true);
      setTranscription("");
      // STT ë¹„í™œì„±í™”
      // startSpeechRecognition();

      if (!socketRef.current) {
        socketRef.current = io(`${BASE_URL}`);
        socketRef.current.on('connect', () => {
          console.log('Socket.IO connected');
          socketRef.current?.emit('join', { userId: user.id });
        });
        socketRef.current.on('sentiment-update', (data: { newScore: string }) => {
          setSentimentScore(parseFloat(data.newScore));
        });
        socketRef.current.on('disconnect', () => {
          console.log('Socket.IO disconnected');
        });
      }
      
      console.log("âœ… ë©´ì ‘ ì‹œì‘ ìš”ì²­ ì „ì†¡ ì™„ë£Œ");

    } catch (err) {
      console.error("âŒ ë©´ì ‘ ì‹œì‘ ìš”ì²­ ì‹¤íŒ¨:", err);
      toast({
        title: "ë©´ì ‘ ì‹œì‘ ì‹¤íŒ¨",
        description: "ì„œë²„ì— ë©´ì ‘ ì‹œì‘ ìš”ì²­ì„ ì „ì†¡í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
        variant: "destructive"
      });
    }
  };

  const toggleRecording = () => {
    const nextState = !isRecording;
    setIsRecording(nextState);
    // STT ë¹„í™œì„±í™”
    // nextState ? startSpeechRecognition() : stopSpeechRecognition();
    toast({
      title: nextState ? "ë©´ì ‘ ì¬ì‹œì‘" : "ë©´ì ‘ ì¼ì‹œì •ì§€",
      description: nextState ? "ë©´ì ‘ì´ ì¬ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤." : "ë©´ì ‘ì´ ì¼ì‹œì •ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
    });
  };

  const handleNextQuestion = async () => {
    if (!user || interviewId === null) return;

    // ì ìˆ˜ ì´ˆê¸°í™” API í˜¸ì¶œ
    try {
      await axios.post(`/api/interview/reset-score`, { interviewId });
      setSentimentScore(100);
    } catch (error) {
      console.error("ì ìˆ˜ ì´ˆê¸°í™” ì‹¤íŒ¨:", error);
      toast({
        title: "ì ìˆ˜ ì´ˆê¸°í™” ì‹¤íŒ¨",
        description: "ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ë„˜ì–´ê°€ê¸° ì „ ì ìˆ˜ë¥¼ ì´ˆê¸°í™”í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
        variant: "destructive"
      });
      // ì ìˆ˜ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í•˜ë”ë¼ë„ ì¼ë‹¨ ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ë„˜ì–´ê°€ë„ë¡ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    }

    const questionText = questions[currentQuestion];
    const answerText = transcription.trim();

    try {
      await axios.post(`/api/interview/response`, {
        interviewId,
        questionNumber: currentQuestion + 1,
        questionText: questionText,
        answerText: answerText
      });
      console.log(`âœ… ì§ˆë¬¸ ì „ì†¡ ì™„ë£Œ: ${questionText}`);
    } catch (err) {
      console.error("âŒ ì‘ë‹µ ì „ì†¡ ì‹¤íŒ¨:", err);
      toast({
        title: "ì „ì†¡ ì‹¤íŒ¨",
        description: "ì‘ë‹µ ë°ì´í„°ë¥¼ ì„œë²„ë¡œ ì „ì†¡í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
        variant: "destructive"
      });
    }

    
    if (currentQuestion < questions.length - 1) {
      setCurrentQuestion(currentQuestion + 1);
      setTimeRemaining(180);
      setTranscription("");
      toast({
        title: "ë‹¤ìŒ ì§ˆë¬¸",
        description: `ì§ˆë¬¸ ${currentQuestion + 2}ë²ˆìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.`
      });
    } else {
      toast({
        title: "ë©´ì ‘ ì™„ë£Œ!",
        description: "ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."
      });
      stopSpeechRecognition();
      socketRef.current?.disconnect();
      setTimeout(() => navigate("/results/1"), 2000);
    }
  };

  const handleInterviewFinish = async () => {
    if (!user) return;

    try {
      await fetch(`${BASE_URL}/api/interview/finish`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ user_id: user.id })
      });

      toast({
        title: "ë©´ì ‘ ì¢…ë£Œ",
        description: "ê²°ê³¼ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤..."
      });

      stopSpeechRecognition();
      socketRef.current?.disconnect();
      setTimeout(() => navigate("/results/1"), 2000);
    } catch (err) {
      console.error("âŒ ì¸í„°ë·° ì¢…ë£Œ ìš”ì²­ ì‹¤íŒ¨:", err);
      toast({
        title: "ì¢…ë£Œ ì‹¤íŒ¨",
        description: "ì„œë²„ì— ì¢…ë£Œ ìš”ì²­ì„ ì „ì†¡í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
        variant: "destructive"
      });
    }
  };


  const toggleVideo = () => setIsVideoOn(!isVideoOn);
  const toggleMic = () => setIsMicOn(!isMicOn);
  const formatTime = (seconds: number) => `${Math.floor(seconds / 60)}:${(seconds % 60).toString().padStart(2, '0')}`;

  return (
    <div className="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8">
      {/* Header */}
      <div className="mb-6">
        <div className="flex items-center justify-between">
          <div>
            <h1 className="text-2xl font-bold text-slate-900">AI ë©´ì ‘ ì§„í–‰</h1>
            <p className="text-slate-600">ì‹¤ì œ ë©´ì ‘ì²˜ëŸ¼ ì§„í–‰ë©ë‹ˆë‹¤. ì¹¨ì°©í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.</p>
          </div>
          <div className="flex items-center gap-4">
            {isInterviewStarted && (
              <Badge variant="outline" className="text-sm">
                ì§ˆë¬¸ {currentQuestion + 1} / {questions.length}
              </Badge>
            )}
            {isInterviewStarted && (
              <div className="flex items-center gap-2 text-lg font-mono">
                <Clock className="h-5 w-5" />
                {formatTime(timeRemaining)}
              </div>
            )}
          </div>
        </div>
        <Progress value={((currentQuestion + 1) / questions.length) * 100} className="mt-4 h-2" />
      </div>

      {/* Current Question */}
      <div className="mb-8">
        <Card className="border-2 border-blue-200 bg-gradient-to-r from-blue-50 to-indigo-50">
          <CardHeader className="pb-4">
            <CardTitle className="flex items-center text-blue-700">
              <HelpCircle className="mr-3 h-6 w-6" />
              {isInterviewStarted ? `ì§ˆë¬¸ ${currentQuestion + 1}` : "ë©´ì ‘ ì¤€ë¹„"}
            </CardTitle>
          </CardHeader>
          <CardContent>
            <div className="text-center">
              <p className="text-xl md:text-2xl font-semibold leading-relaxed text-slate-800 mb-6">
                {isInterviewStarted
                  ? questions.length > 0
                    ? questions[currentQuestion]
                    : "ë©´ì ‘ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. ì„¤ì • í˜ì´ì§€ì—ì„œ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”."
                  : "ë©´ì ‘ ì‹œì‘ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì´ê³³ì—ì„œ ì§ˆë¬¸ì´ ë‚˜ì˜µë‹ˆë‹¤."}
              </p>
              <div className="p-4 bg-blue-100 rounded-lg border border-blue-200">
                <p className="text-base text-blue-800">
                  ğŸ’¡ <strong>ë‹µë³€ íŒ:</strong> êµ¬ì²´ì ì¸ ê²½í—˜ê³¼ ê²°ê³¼ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ë©´ ë” ì¢‹ì€ í‰ê°€ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                </p>
              </div>
            </div>
          </CardContent>
        </Card>
      </div>

      <div className="grid lg:grid-cols-3 gap-8">
        {/* Video and Controls */}
        <div className="lg:col-span-2">
          <Card>
            <CardHeader>
              <CardTitle className="flex items-center justify-between">
                <span>ë©´ì ‘ í™”ë©´</span>
                <div className="flex gap-2">
                  <Button variant="outline" size="sm" onClick={toggleVideo} className={isVideoOn ? "" : "bg-red-50 border-red-200"}>
                    {isVideoOn ? <Video className="h-4 w-4" /> : <VideoOff className="h-4 w-4" />}
                  </Button>
                  <Button variant="outline" size="sm" onClick={toggleMic} className={isMicOn ? "" : "bg-red-50 border-red-200"}>
                    {isMicOn ? <Mic className="h-4 w-4" /> : <MicOff className="h-4 w-4" />}
                  </Button>
                  <Button variant="outline" size="sm">
                    <Settings className="h-4 w-4" />
                  </Button>
                </div>
              </CardTitle>
            </CardHeader>
            <CardContent>
              <div className="relative bg-slate-900 rounded-lg overflow-hidden">
                <video 
                  ref={videoRef} 
                  autoPlay 
                  muted 
                  className="w-full h-64 md:h-80 object-cover" 
                  style={{ display: isVideoOn ? 'block' : 'none' }}
                  onLoadedData={() => setIsReadyToAnalyze(true)}
                />
                {!isVideoOn && (
                  <div className="w-full h-64 md:h-80 flex items-center justify-center text-white">
                    <div className="text-center">
                      <VideoOff className="h-12 w-12 mx-auto mb-2 opacity-50" />
                      <p className="text-sm opacity-75">ì¹´ë©”ë¼ê°€ êº¼ì ¸ìˆìŠµë‹ˆë‹¤</p>
                    </div>
                  </div>
                )}
                {isRecording && (
                  <div className="absolute top-4 left-4 flex items-center gap-2 bg-red-600 text-white px-3 py-1 rounded-full text-sm">
                    <div className="w-2 h-2 bg-white rounded-full animate-pulse" />
                    ë…¹í™” ì¤‘
                  </div>
                )}
                <div className="absolute top-4 right-4 bg-black/50 text-white px-3 py-1 rounded-full text-sm">
                  ê°ì • ì ìˆ˜: {sentimentScore.toFixed(2)}
                </div>
              </div>

              <div className="flex justify-center gap-4 mt-6">
                {!isInterviewStarted ? (
                  <Button onClick={startInterview} size="lg" className="bg-blue-600 hover:bg-blue-700">
                    <Play className="mr-2 h-5 w-5" />
                    ë©´ì ‘ ì‹œì‘
                  </Button>
                ) : (
                  <>
                    <Button onClick={toggleRecording} variant={isRecording ? "destructive" : "default"} size="lg">
                      {isRecording ? (<><Pause className="mr-2 h-5 w-5" />ì¼ì‹œì •ì§€</>) : (<><Play className="mr-2 h-5 w-5" />ì¬ì‹œì‘</>)}
                    </Button>

                    {currentQuestion < questions.length - 1 ? (
                      <Button onClick={handleNextQuestion} variant="outline" size="lg">
                        <SkipForward className="mr-2 h-5 w-5" />
                        ë‹¤ìŒ ì§ˆë¬¸
                      </Button>
                    ) : (
                      <Button onClick={handleInterviewFinish} variant="outline" size="lg">
                        <SkipForward className="mr-2 h-5 w-5" />
                        ì¸í„°ë·° ì¢…ë£Œ
                      </Button>
                    )}
                  </>
                )}
              </div>
            </CardContent>
          </Card>
        </div>

        {/* Transcription */}
        <div>
          <Card>
            <CardHeader>
              <CardTitle>ë‹µë³€ ì…ë ¥</CardTitle>
            </CardHeader>
            <CardContent>
              <Textarea
                value={transcription}
                onChange={(e) => setTranscription(e.target.value)}
                placeholder="ì´ê³³ì— ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”. STTê°€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤."
                className="min-h-32 text-base"
                disabled={!isRecording}
              />
              <div className="flex justify-end gap-2 mt-4">
                <Button variant="ghost" onClick={() => setTranscription('')}>ì´ˆê¸°í™”</Button>
                <Button onClick={handleNextQuestion}>ì…ë ¥</Button>
              </div>
            </CardContent>
          </Card>
        </div>
      </div>
    </div>
  );
};

export default Interview;


